{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce15a4a9",
   "metadata": {},
   "source": [
    "# Biebrza Pixel Time-Series Classification with PyTorch\n",
    "\n",
    "This notebook loads the exported **biannual pixel time-series** dataset from Google Drive\n",
    "and trains two neural models to classify vegetation trajectory categories:\n",
    "\n",
    "- A **1D Convolutional Network** (Conv1D)\n",
    "- An **LSTM**\n",
    "\n",
    "It then compares their performance.\n",
    "\n",
    "Assumptions:\n",
    "- This notebook loads a table from Google Earth Engine named\n",
    "  `biebrza_biannual_pixel_series.csv` from the folder `GEE_Biebrza` in your Drive.\n",
    "- Each row is one pixel, with columns:\n",
    "  - `NDMI_YYYY_YYYY`, `NBR_YYYY_YYYY`, `NIR_YYYY_YYYY` for multiple 2-year windows\n",
    "  - `traj_simpl` (original category label)\n",
    "  - `numark` (square id)\n",
    "  - `pixel_id` (unique pixel id)\n",
    "  - `cat_id` (original numeric label, can be ignored here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdead141",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision torchaudio scikit-learn\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.utils import shuffle as sk_shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32549165",
   "metadata": {},
   "source": [
    "## 1. Load dataset from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff984d29",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:\\\\git_projects\\\\biebrza-shrub-encroachment-analysis\\\\output\\\\kopec_and_slawik_2020_pixel_series\\\\biebrza_biannual_pixel_series.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-456035652.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'E:\\git_projects\\biebrza-shrub-encroachment-analysis\\output\\kopec_and_slawik_2020_pixel_series\\biebrza_biannual_pixel_series.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data shape:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Columns:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:\\\\git_projects\\\\biebrza-shrub-encroachment-analysis\\\\output\\\\kopec_and_slawik_2020_pixel_series\\\\biebrza_biannual_pixel_series.csv'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Path to your exported CSV (adjust if needed)\n",
    "data_path = '/content/drive/MyDrive/GEE_Biebrza/biebrza_biannual_pixel_series.csv'\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "print('Data shape:', df.shape)\n",
    "print('Columns:', df.columns.tolist()[:20], '...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9534d9a8",
   "metadata": {},
   "source": [
    "Quick look at the main label columns and one row:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29460521",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['traj_simpl', 'numark']].head())\n",
    "print('\\nUnique traj_simpl values:', df['traj_simpl'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d74789",
   "metadata": {},
   "source": [
    "## 2. Re-label categories and subsample pixels\n",
    "\n",
    "Steps:\n",
    "1. Merge `wetland_to_trees` into `wetland_to_shrubs` â†’ new class `wetland_to_woody`.\n",
    "2. Keep only these classes:\n",
    "   - `wetland_to_woody` (all pixels)\n",
    "   - `shrubs_to_trees` (all pixels)\n",
    "   - `stable_wetland` (max 5000 pixels)\n",
    "   - `stable_trees` (max 5000 pixels)\n",
    "   - `stable_shrubs` (max 5000 pixels)\n",
    "3. Drop all other categories.\n",
    "4. Compute **class weights** based on the full dataset size (before subsampling)\n",
    "   for use in the loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2de7802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Create unified class string column\n",
    "\n",
    "df['class_str'] = df['traj_simpl']\n",
    "\n",
    "# Merge wetland_to_* into wetland_to_woody\n",
    "df.loc[df['class_str'].isin(['wetland_to_shrubs', 'wetland_to_trees']), 'class_str'] = 'wetland_to_woody'\n",
    "\n",
    "print('New unique class_str values:', df['class_str'].unique())\n",
    "\n",
    "# 2.2 Keep only desired classes\n",
    "target_classes = ['wetland_to_woody', 'shrubs_to_trees', 'stable_wetland', 'stable_trees', 'stable_shrubs']\n",
    "\n",
    "df = df[df['class_str'].isin(target_classes)].copy()\n",
    "print('After filtering, shape:', df.shape)\n",
    "print('Class counts (all pixels, before subsampling):')\n",
    "print(df['class_str'].value_counts())\n",
    "\n",
    "# 2.3 Compute class weights based on FULL filtered dataset size\n",
    "class_counts_full = df['class_str'].value_counts().to_dict()\n",
    "num_classes = len(target_classes)\n",
    "total_samples_full = len(df)\n",
    "\n",
    "class_weights_dict = {}\n",
    "for cls in target_classes:\n",
    "    count_c = class_counts_full.get(cls, 1)\n",
    "    # Inverse-frequency style weight\n",
    "    class_weights_dict[cls] = total_samples_full / (num_classes * count_c)\n",
    "\n",
    "print('\\nClass weights (for loss):')\n",
    "for k, v in class_weights_dict.items():\n",
    "    print(f'  {k}: {v:.3f}')\n",
    "\n",
    "# 2.4 Subsample per class (where requested)\n",
    "samples_per_class = {\n",
    "    'wetland_to_woody': None,   # all\n",
    "    'shrubs_to_trees': None,   # all\n",
    "    'stable_wetland': 5000,\n",
    "    'stable_trees': 5000,\n",
    "    'stable_shrubs': 5000,\n",
    "}\n",
    "\n",
    "dfs = []\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "for cls in target_classes:\n",
    "    df_cls = df[df['class_str'] == cls]\n",
    "    n = len(df_cls)\n",
    "    max_n = samples_per_class[cls]\n",
    "    if max_n is None or n <= max_n:\n",
    "        dfs.append(df_cls)\n",
    "        print(f'Class {cls}: using ALL {n} pixels')\n",
    "    else:\n",
    "        df_sampled = df_cls.sample(n=max_n, random_state=42)\n",
    "        dfs.append(df_sampled)\n",
    "        print(f'Class {cls}: downsampled from {n} to {max_n} pixels')\n",
    "\n",
    "df_balanced = pd.concat(dfs, ignore_index=True)\n",
    "df_balanced = sk_shuffle(df_balanced, random_state=42)\n",
    "\n",
    "print('\\nAfter balancing, per-class counts:')\n",
    "print(df_balanced['class_str'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e901987",
   "metadata": {},
   "source": [
    "## 3. Train/Val/Test split by square (`numark`) and class\n",
    "\n",
    "Requirements:\n",
    "- 60% train, 20% validation, 20% test.\n",
    "- Stratified by **class_str**.\n",
    "- All pixels from a given `numark` (square id) must go to the same split.\n",
    "\n",
    "Implementation:\n",
    "- For each class, collect unique `numark` values.\n",
    "- Within each class, split its `numark`s into train/val/test sets by the 60/20/20 rule.\n",
    "- Union over classes to get global sets of `numark` for each split.\n",
    "  (Assumes each `numark` belongs predominantly to one class, as is typical for MPC squares.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7812cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Stratified group split by class_str and numark\n",
    "\n",
    "train_squares = set()\n",
    "val_squares = set()\n",
    "test_squares = set()\n",
    "\n",
    "rng = np.random.default_rng(123)\n",
    "\n",
    "for cls in target_classes:\n",
    "    df_cls = df_balanced[df_balanced['class_str'] == cls]\n",
    "    squares = df_cls['numark'].dropna().unique()\n",
    "    squares = list(squares)\n",
    "    rng.shuffle(squares)\n",
    "\n",
    "    n = len(squares)\n",
    "    n_train = int(0.6 * n)\n",
    "    n_val = int(0.2 * n)\n",
    "    # rest to test\n",
    "\n",
    "    train_s = squares[:n_train]\n",
    "    val_s = squares[n_train:n_train + n_val]\n",
    "    test_s = squares[n_train + n_val:]\n",
    "\n",
    "    train_squares.update(train_s)\n",
    "    val_squares.update(val_s)\n",
    "    test_squares.update(test_s)\n",
    "\n",
    "print('Unique train squares:', len(train_squares))\n",
    "print('Unique val squares  :', len(val_squares))\n",
    "print('Unique test squares :', len(test_squares))\n",
    "\n",
    "# 3.2 Build the actual splits\n",
    "is_train = df_balanced['numark'].isin(train_squares)\n",
    "is_val = df_balanced['numark'].isin(val_squares)\n",
    "is_test = df_balanced['numark'].isin(test_squares)\n",
    "\n",
    "df_train = df_balanced[is_train].copy()\n",
    "df_val = df_balanced[is_val].copy()\n",
    "df_test = df_balanced[is_test].copy()\n",
    "\n",
    "print('\\nSplit sizes (rows):')\n",
    "print('Train:', df_train.shape[0])\n",
    "print('Val  :', df_val.shape[0])\n",
    "print('Test :', df_test.shape[0])\n",
    "\n",
    "print('\\nPer-class counts in Train:')\n",
    "print(df_train['class_str'].value_counts())\n",
    "print('\\nPer-class counts in Val:')\n",
    "print(df_val['class_str'].value_counts())\n",
    "print('\\nPer-class counts in Test:')\n",
    "print(df_test['class_str'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db553cf",
   "metadata": {},
   "source": [
    "## 4. Build time-series tensors for PyTorch\n",
    "\n",
    "We:\n",
    "- Identify all NDMI/NBR/NIR band columns.\n",
    "- Sort them by name to keep time order.\n",
    "- Reshape features into `[N, T, C]` with `C=3` (NDMI, NBR, NIR).\n",
    "- Encode `class_str` to integer labels 0..(num_classes-1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625e8111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Identify time-series columns (NDMI/NBR/NIR)\n",
    "\n",
    "ts_cols = sorted([c for c in df_balanced.columns\n",
    "                 if c.startswith('NDMI_') or c.startswith('NBR_') or c.startswith('NIR_')])\n",
    "\n",
    "print('Number of time-series columns:', len(ts_cols))\n",
    "print('First 9 time-series cols:', ts_cols[:9])\n",
    "\n",
    "# 4.2 Determine sequence length (T) and feature channels (C=3)\n",
    "T = len(ts_cols) // 3\n",
    "assert len(ts_cols) == 3 * T, 'Expected number of time-series columns to be multiple of 3.'\n",
    "\n",
    "print(f'Sequence length T = {T}, channels C = 3')\n",
    "\n",
    "# 4.3 Label encoding\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(target_classes)  # ensure consistent class order\n",
    "\n",
    "df_train['label_idx'] = le.transform(df_train['class_str'])\n",
    "df_val['label_idx'] = le.transform(df_val['class_str'])\n",
    "df_test['label_idx'] = le.transform(df_test['class_str'])\n",
    "\n",
    "print('Label mapping:')\n",
    "for cls, idx in zip(le.classes_, range(len(le.classes_))):\n",
    "    print(f'  {cls} -> {idx}')\n",
    "\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "# 4.4 Convert class_weights_dict (string->float) into tensor aligned with label_idx\n",
    "class_weights_list = []\n",
    "for cls in le.classes_:\n",
    "    class_weights_list.append(class_weights_dict[cls])\n",
    "\n",
    "class_weights = torch.tensor(class_weights_list, dtype=torch.float32)\n",
    "print('\\nClass weights tensor:', class_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcbce41",
   "metadata": {},
   "source": [
    "## 5. PyTorch Dataset and DataLoaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8e6366",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df, ts_cols, label_col):\n",
    "        X = df[ts_cols].values.astype(np.float32)\n",
    "        N = X.shape[0]\n",
    "        T = len(ts_cols) // 3\n",
    "        # reshape to [N, T, 3]\n",
    "        X = X.reshape(N, T, 3)\n",
    "        self.X = X\n",
    "        self.y = df[label_col].values.astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "train_ds = PixelTimeSeriesDataset(df_train, ts_cols, 'label_idx')\n",
    "val_ds = PixelTimeSeriesDataset(df_val, ts_cols, 'label_idx')\n",
    "test_ds = PixelTimeSeriesDataset(df_test, ts_cols, 'label_idx')\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b612e9c",
   "metadata": {},
   "source": [
    "## 6. Define models: 1D-Conv and LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead76670",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "class Conv1DClassifier(nn.Module):\n",
    "    def __init__(self, seq_len, num_classes):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)  # global average pooling over time\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, C]\n",
    "        x = x.permute(0, 2, 1)  # [B, C, T]\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x).squeeze(-1)  # [B, 64]\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=False)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, C]\n",
    "        out, (h_n, c_n) = self.lstm(x)  # h_n: [num_layers, B, H]\n",
    "        last_hidden = h_n[-1]           # [B, H]\n",
    "        logits = self.fc(last_hidden)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f277099",
   "metadata": {},
   "source": [
    "## 7. Training and evaluation utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13a9020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for X, y in loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * X.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += X.size(0)\n",
    "\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for X, y in loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        total_loss += loss.item() * X.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += X.size(0)\n",
    "\n",
    "    return total_loss / total, correct / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a031ae",
   "metadata": {},
   "source": [
    "## 8. Train Conv1D model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a255b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = T\n",
    "conv_model = Conv1DClassifier(seq_len=seq_len, num_classes=num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "optimizer = torch.optim.Adam(conv_model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_acc = train_one_epoch(conv_model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = evaluate(conv_model, val_loader, criterion, device)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state = conv_model.state_dict()\n",
    "\n",
    "    print(f'Epoch {epoch:02d}: '\n",
    "          f'train_loss={train_loss:.4f}, train_acc={train_acc:.3f}, '\n",
    "          f'val_loss={val_loss:.4f}, val_acc={val_acc:.3f}')\n",
    "\n",
    "print('\\nBest Conv1D val_acc:', best_val_acc)\n",
    "\n",
    "if best_state is not None:\n",
    "    conv_model.load_state_dict(best_state)\n",
    "\n",
    "test_loss_conv, test_acc_conv = evaluate(conv_model, test_loader, criterion, device)\n",
    "print(f'Conv1D TEST: loss={test_loss_conv:.4f}, acc={test_acc_conv:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0b5753",
   "metadata": {},
   "source": [
    "## 9. Train LSTM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7638b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTMClassifier(input_size=3, hidden_size=64, num_layers=1, num_classes=num_classes).to(device)\n",
    "\n",
    "criterion_lstm = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "optimizer_lstm = torch.optim.Adam(lstm_model.parameters(), lr=1e-3)\n",
    "\n",
    "best_val_acc_lstm = 0.0\n",
    "best_state_lstm = None\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_acc = train_one_epoch(lstm_model, train_loader, criterion_lstm, optimizer_lstm, device)\n",
    "    val_loss, val_acc = evaluate(lstm_model, val_loader, criterion_lstm, device)\n",
    "\n",
    "    if val_acc > best_val_acc_lstm:\n",
    "        best_val_acc_lstm = val_acc\n",
    "        best_state_lstm = lstm_model.state_dict()\n",
    "\n",
    "    print(f'Epoch {epoch:02d}: '\n",
    "          f'train_loss={train_loss:.4f}, train_acc={train_acc:.3f}, '\n",
    "          f'val_loss={val_loss:.4f}, val_acc={val_acc:.3f}')\n",
    "\n",
    "print('\\nBest LSTM val_acc:', best_val_acc_lstm)\n",
    "\n",
    "if best_state_lstm is not None:\n",
    "    lstm_model.load_state_dict(best_state_lstm)\n",
    "\n",
    "test_loss_lstm, test_acc_lstm = evaluate(lstm_model, test_loader, criterion_lstm, device)\n",
    "print(f'LSTM TEST: loss={test_loss_lstm:.4f}, acc={test_acc_lstm:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a7095f",
   "metadata": {},
   "source": [
    "## 10. Final comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12181238",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n=== Final Test Results ===')\n",
    "print(f'Conv1D  - test_acc: {test_acc_conv:.3f}')\n",
    "print(f'LSTM    - test_acc: {test_acc_lstm:.3f}')\n",
    "\n",
    "print('\\nClass mapping (label_idx -> class_str):')\n",
    "for idx, cls in enumerate(le.classes_):\n",
    "    print(f'  {idx}: {cls}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
